suppressWarnings(library(tm))
suppressWarnings(library(RWeka))
library(rJava)
library(data.table)
library(Matrix)
library(SnowballC)
path='/Users/daxinggao/Downloads/final 2/en_US'
path1=file.path(path,"en_US.blogs.txt")
path2=file.path(path,"en_US.news.txt")
path3=file.path(path,"en_US.twitter.txt")
fh1=file(path1,open="r")
fh2=file(path2,open="r")
fh3=file(path3,open="r")

set.seed(1)
F1=readLines(fh1,warn=F)
F1=sample(F1,as.integer(NROW(F1)*0.15),replace=F)
close(fh1)
F1=iconv(F1,"latin1", "ASCII", sub="")

F2=readLines(fh2,warn=F)
set.seed(1)
F2=sample(F2,as.integer(NROW(F2)*0.15),replace=F)
close(fh2)
F2=iconv(F2,"latin1", "ASCII", sub="")

F3=readLines(fh3,warn=F)
set.seed(1)
F3=sample(F3,as.integer(NROW(F3)*0.15),replace=F)
close(fh3)
F3=iconv(F3,"latin1", "ASCII", sub="")##remove emoji from twitter

fprofane=file("/Users/daxinggao/Desktop/profanity.txt")
profane=VectorSource(readLines(fprofane,warn=F))
close(fprofane)
removeURL <- function(x) gsub("(ht|f)tp[[:alnum:][:punct:]]*", "", x)
removeshortword<-function(x)gsub("(\\s|^)([a-z]{1,2}(\\s|$))+"," ",x)
removeEmail<-function(x)gsub("[[:alnum:][:punct:]]*@[[:alnum:][:punct:]]*","",x)
removeWWW<-function(x)gsub("www\\.[[:alnum:][:punct:]]*","",x)
removehastag<-function(x)gsub("\\#[[:alnum:][:punct:]]*","",x)
removerepeatletter<-function(x)gsub("([[:alpha:]])\\1{2,}","\\1",x)
replacepunctuation<-function(x)gsub("[[:punct:]]+"," ",x)
shownot<-function(x)gsub("n\\'t"," not",x)

unifreq=NULL
bifreq=NULL
trifreq=NULL
quanfreq=NULL
options(mc.cores=1)


  t=Sys.time()
  docs=Corpus(VectorSource(c(f1,f2,f3)))
  docs=tm_map(docs,content_transformer(tolower))
  docs=tm_map(docs, content_transformer(removeURL))
  docs=tm_map(docs, content_transformer(removeEmail))
  docs=tm_map(docs, content_transformer(removehastag))
  docs=tm_map(docs, content_transformer(removeWWW))
  
  docs=tm_map(docs,stripWhitespace)
  docs=tm_map(docs, content_transformer(removerepeatletter))
  docs=tm_map(docs,removeWords,c(stopwords("english"),"can","will"))
  docs=tm_map(docs,removeWords,profane$content[-1])
  
  docs=tm_map(docs,content_transformer(replacepunctuation))
  docs=tm_map(docs, removeNumbers)
  docs=tm_map(docs, content_transformer(removeshortword))
  ##docs=tm_map(docs,content_transformer(shownot))
  docs=tm_map(docs,stripWhitespace)
  ##docs=tm_map(docs,content_transformer(correctmis))
  ##docs=tm_map(docs, stemDocument)
  docs=tm_map(docs,PlainTextDocument)
  
  parsegram<-function(gram){
  
  if(gram==1){
  
  UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))##build unigram
  tdm1<-TermDocumentMatrix(docs,control = list(tokenize=UnigramTokenizer))
  sm1_f=Matrix::rowSums(sparseMatrix(i=tdm1$i, j=tdm1$j, x=tdm1$v))
  sm1_dt=cbind(tdm1$dimnames$Terms,sm1_f)
  tdm1m=data.table(sm1_dt)
  cat("size is ",object.size(tdm1m)/1024^2," MB\n")
  cat("One time excluded is ",object.size(tdm1m[sm1_f>1])/1024^2," MB\n")
  write.csv(tdm1m[sm1_f>1],"uni_http_N.csv")
  }
  
  else if(gram==2){
  
  BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))##build bigram
  tdm2<-TermDocumentMatrix(docs,control = list(tokenize=BigramTokenizer))
  sm2_f=Matrix::rowSums(sparseMatrix(i=tdm2$i, j=tdm2$j, x=tdm2$v))
  sm2_dt=cbind(tdm2$dimnames$Terms,sm2_f)
  tdm2m=data.table(sm2_dt)
  cat("size is ",object.size(tdm2m)/1024^2," MB\n")
  cat("One time excluded is ",object.size(tdm2m[sm2_f>1])/1024^2," MB\n")
  write.csv(tdm2m[sm2_f>1],"bi_http_N.csv")
  
  }
  
  else if(gram==3){
  TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))##build trigram
  tdm3<-TermDocumentMatrix(docs,control = list(tokenize=TrigramTokenizer))
  sm3_f=Matrix::rowSums(sparseMatrix(i=tdm3$i, j=tdm3$j, x=tdm3$v))
  sm3_dt=cbind(tdm3$dimnames$Terms,sm3_f)
  tdm3m=data.table(sm3_dt)
  cat("size is ",object.size(tdm3m)/1024^2," MB\n")
  cat("One time excluded is ",object.size(tdm3m[sm3_f>1])/1024^2," MB\n")
  write.csv(tdm3m[sm3_f>1],"tri_http_N.csv")
  }
  
  else if(gram==4){
  QuangramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
  tdm4<-TermDocumentMatrix(docs,control = list(tokenize=QuangramTokenizer))
  sm4_f=Matrix::rowSums(sparseMatrix(i=tdm4$i, j=tdm4$j, x=tdm4$v))
  sm4_dt=cbind(tdm4$dimnames$Terms,sm4_f)
  tdm4m=data.table(sm4_dt)
  cat("size is ",object.size(tdm4m)/1024^2," MB\n")
  cat("One time excluded is ",object.size(tdm4m[sm4_f>1])/1024^2," MB\n")
  write.csv(tdm4m[sm4_f>1],"quan_http_N.csv")
}
  print(Sys.time()-t)
  
}
}

